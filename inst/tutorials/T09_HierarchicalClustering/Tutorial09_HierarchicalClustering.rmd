---
title: "Introduction to Hierarchical Clustering"
author:
- name: Pete E. Pascuzzi, Mingxuan Zhang
  affiliation: Purdue University
runtime: shiny_prerendered
output: learnr::tutorial
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(tidyr)
library(datasets)
library(pheatmap)
library(magrittr)
library(ggplot2)
library(RColorBrewer)
library(cluster)
library(kableExtra)
knitr::opts_chunk$set(fig.width=6)
knitr::opts_chunk$set(fig.height=6)
iris <- as_tibble(iris) %>%
  mutate(plant_id = str_c("plant", 1:150, sep="_"))

ex_data1 <- data.frame(name=c("A", "B","C", "D"),
                      var1=c(1, 1.5, 10, 12),
                      var2=c(1, 2, 8, 11))

ex_data2 <- tibble(height = c(6, 8, 6),
                   weight = c(95, 100,99),
                   name = c("Chris", "Eric", "Adam"))
```


## Introduction

Hierarchical clustering is a method of unsupervisied learning that can discover patterns in a data set. These patterns can be used to group similar observations for further analyses.

Recall the `iris` data set which was introduced in previous tutorial.

```{r q01, exercise=TRUE, exercise.startover=TRUE}
iris <- as_tibble(iris) %>%
  mutate(plant_id = str_c("plant", 1:150, sep="_"))
iris
```

This data set contains information for 3 types of iris plant: **Setosa**, **Versicolor**, **Virginica**. We hope that we can use the hierarchical clustering algorithm to group the similar iris plants, i.e. plants that belong to same species.

In this tutorial, you will learn how hierarchical clustering works by studying it's cruical components and how to apply it to real data set like `iris`. So let's get into it!

## Learning Objectives

* Describe the **Euclidean** and **Manhattan** distances between two observations.  
* Describe the differences between **Average**, **Complete** and **Single** linkage methods.  
* Explain why variables should be scaled before clustering occurs.  
* Describe **agglomerative clustering** and how distances and linkage methods are used in the process.  
* Explain how to interpret a dendrogram.  
* Create a **heatmap** and describe how to interpret it.  

## Distance Measures

If you want to compare two observations, you must have a method to combine the variables for these observations into a single value that describes the similarity between these observations.  Generally, the similarity is expressed as a dissimilarity or **distance** where a **distance** of zero indicates that the observations are identical (at least for the given variables).  

There are many ways to determine **distance**. Two commonly used metrics for numeric data are **Euclidean distance** and **Manhattan distance**. Both of these are fairly intuitive and can be described with geometry.  

The following are the definitions for these two metrics:

Suppose that $x$ and $y$ are two observations that have the same variables: $\{\text{var}_1,\text{var}_2,\dots,\text{var}_n\}$, and their variables' values are $\{x_1, x_2, x_3,\dots,x_n\}$ and $\{y_1, y_2,\dots,y_n\}$ respectively. Then, the two distance measures are calculated as following:

\[ \text{Euclidean distance($x$, $y$)} = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2} \]

\[ \text{Manhattan distance($x$, $y$)} = \sum_{i=1}^{n} |x_i-y_i| \]

Let's use three observations from `iris` to illustrate this.  

```{r prepare-data1, include=FALSE}
iris_sub <- iris %>%
  slice(c(26, 74, 110))
```


```{r, q11, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1"}
iris_sub <- iris %>%
  slice(c(26, 74, 110))
iris_sub
```

This subset of the data has one observation for each species.  Let's plot only `Petal.Length` and `Petal.Width`.  

```{r, q11a, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1"}
iris_plot <- iris_sub %>%
  ggplot() + 
  geom_point(mapping=aes(x=Petal.Length, y=Petal.Width, color=plant_id), size=3) + 
  geom_text(mapping=aes(x=Petal.Length, y=Petal.Width, label=plant_id), position=position_nudge(x=0.8, y=0.1)) +
  xlim(c(0, 8)) +
  guides(color=FALSE)
iris_plot
```

From this plot, you should see that, geometrically speakinig, `plant_26` is closer to `plant_74` than to `plant_110`.  From this plot, it is difficult to determine which plant is closest to `plant_74`.  It could be either `plant_26` or `plant_110`.  

Let's annotate the plot further to illustrate how we can calculate the above distances.  

```{r, prepare-data1a, include=FALSE}
iris_sub <- iris %>%
  slice(c(26, 74, 110))

iris_plot <- iris_sub %>%
  ggplot() + 
  geom_point(mapping=aes(x=Petal.Length, y=Petal.Width, color=plant_id), 
             size=3) + 
  geom_text(mapping=aes(x=Petal.Length, y=Petal.Width, label=plant_id), 
            position=position_nudge(x=0.8, y=0.1)) +
  xlim(c(0, 8)) +
  guides(color=FALSE)
```

```{r, q11b, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
x_coords <- iris_sub %>%
  slice(c(1, 2, 3)) %>%
  pull(Petal.Length)
y_coords <- iris_sub %>%
  slice(c(1, 2, 3)) %>%
  pull(Petal.Width)

iris_plot +
  annotate("segment", x=x_coords[c(1, 2, 2, 3)], 
           y=y_coords[c(1, 2, 1, 2)], 
           xend=x_coords[c(2, 3, 2, 3)], 
           yend=y_coords[c(1, 2, 2, 3)], 
           linetype="dashed", 
           color="black") +
  annotate("segment", x=x_coords[c(1, 2)], 
           y=y_coords[c(1, 2)], 
           xend=x_coords[c(2, 3)], 
           yend=y_coords[c(2, 3)], 
           linetype="dotted", 
           color="black") +
  annotate("text", x=c(3.2, 5, 3, 5.5, 6.5, 5.2), 
           y= c(0.1, 0.7, 0.8, 1.1, 1.7, 1.9), 
           label=c("A", "B", "C", "D", "E", "F"), 
           size=5) +
  guides(color=FALSE)
```

Line segments show the distances between `plant_26` and `plant_74`, and the distances between `plant_74` and `plant_110`.  The distances between `plant_26` and `plant_110` are omitted for the sake of clarity.  

The **Euclidean distance** between `plant_26` and `plant_74` is represented by **C**.  It is the shortest distance between the two points because you simply traverse the diagonal between these points.  This diagonal is the hypotenuse of the triangle formed by segments **A**, **B** and **C**.  You can use the Pythagorean Theorem to determine **C** from the lengths **A** and **B**.  

\begin{align}
&A = 4.7 - 1.6 = 3.1\\
&B = 1.2 - 0.2  = 1.0\\
&\text{Euclidean distance(C, D)} = C = \sqrt{A^2 + B^2}
\end{align} 

```{r, q11c, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
A <- 4.7 - 1.6
B <- 1.2 - 0.2
C <- (A^2 + B^2)^0.5
C
```

Use the chunk below to determine the Euclidean Distance between `plant_74` and `plant_110`.  

```{r, q11d, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
E
D
F
F
```

The **Manhattan Distance** is represented by horizontal and vertical distances between the plants.  You cannot traverse the diagonal but must follow the distances of **A** and **B**.  The analogy to remember is walking along the streets of Manhattan.  Generally, you cannot cut diagonally across a city block, you must follow the sidewalk of streets.  

The **Manhattan Distance** between `plant_26` and `plant_74` is the sum of **A** and **B**.  Remember to take the absolute value before you add the distances!  

```{r, q11e, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
A <- abs(4.7 - 1.6)
B <- abs(1.2 - 0.2)
A + B
```

In our simple example, we could determine the distances between the samples manually.  What if you have many observations and variables?  

## The `dist` Function  

In **R**,  you can calculate the pairwise distances of many observations and variables with the function `dist`.  The result is an object of class **dist** that is used as input by many other functions.  Read the help menu for `dist` for the available **distance methods**.  

To use `dist`, you should select only the variables that you want to use for the calculation.  With `iris`, do not include variables `plant_id` or `Species`.  

The chunk below will recalculate the Euclidean Distance for the `iris` subset.  Note, that we are only using two variables.  

```{r q12, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
iris_dist_mat <- iris_sub %>%
  select(Petal.Length:Petal.Width) %>%
  dist(method="euclidean") %>%
  as.matrix()
rownames(iris_dist_mat) <- colnames(iris_dist_mat) <- pull(iris_sub, plant_id)
iris_dist_mat
```

Our manual calculations are confirmed and show that `plant_74` is more similar to `plant_110` than to `plant_26`.  

We can also recalculate the **Manhattan Distance**.  

```{r q13, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
iris_dist_mat <- iris_sub %>%
  select(Petal.Length:Petal.Width) %>%
  dist(method="manhattan") %>%
  as.matrix()
rownames(iris_dist_mat) <- colnames(iris_dist_mat) <- pull(iris_sub, plant_id)
iris_dist_mat
```

Note, the values of the distances are different, but `plant_74` is still more similar to `plant_110`.  

Finally, what if we use all four variables?  

```{r q14, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
iris_dist_mat <- iris_sub %>%
  select(Sepal.Length:Petal.Width) %>%
  dist(method="euclidean") %>%
  as.matrix()
rownames(iris_dist_mat) <- colnames(iris_dist_mat) <- pull(iris_sub, plant_id)
iris_dist_mat
```

Again, same result but different values.  

## Linkage Method  

With agglomerative hierarchical clustering, observations are joined stepwise to form groups or clusters.  

Examine the following plot.  It should be easy to understand how these six observations were joined to form three clusters.  But, agglomerative clustering proceeds until all observations are formed into a single cluster.  

How can you determine the distance between the red, green and blue clusters?  

```{r, q15, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_newsub %>%
  ggplot() +
  aes(x=Petal.Length, y=Petal.Width, color=Species) +
  geom_point() +
  geom_point(aes(x=1.33, y=0.21), shape=1, size=10, color="red") +
  geom_point(aes(x=4.4, y=1.5), shape=1, size=20, color="green") + 
  geom_point(aes(x=5.1, y=1.9), shape=1, size=20, color="blue")
```

There are many methods to determine the **linkage** between clusters.  Three of the more common are illustrated here.  

With **Complete-Linkage**, the distance between two clusters is the maximum distance between observations in each cluster.  The plot below illustrates **complete-linkage** for the `versicolor` cluster and the other clusters.  The distance between `virginica` and `setosa` is not shown.  

```{r, q16, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

x_coords <- pull(iris_newsub, Petal.Length)
y_coords <- pull(iris_newsub, Petal.Width)
iris_newsub %>%
  ggplot() +
  aes(x=Petal.Length, y=Petal.Width, color=Species) +
  geom_point() +
  geom_point(aes(x=1.33, y=0.21), shape=1, size=10, color="red") +
  geom_point(aes(x=4.4, y=1.5), shape=1, size=20, color="green") + 
  geom_point(aes(x=5.1, y=1.9), shape=1, size=20, color="blue") +
  annotate("segment", x=x_coords[c(4, 2)], y=y_coords[c(4, 2)], 
           xend=x_coords[c(5, 3)], yend=y_coords[c(5, 3)],
           linetype="dashed") +
  ggtitle("Complete-Linkage")
```

With **Single-Linkage**, the distance between two clusters is the minimum distance between observations in each cluster.  The plot below illustrates **single-linkage** for the `versicolor` cluster and the other clusters.  The distance between `virginica` and `setosa` is not shown.  

```{r, q17, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

x_coords <- pull(iris_newsub, Petal.Length)
y_coords <- pull(iris_newsub, Petal.Width)
iris_newsub %>%
  ggplot() +
  aes(x=Petal.Length, y=Petal.Width, color=Species) +
  geom_point() +
  geom_point(aes(x=1.33, y=0.21), shape=1, size=10, color="red") +
  geom_point(aes(x=4.4, y=1.5), shape=1, size=20, color="green") + 
  geom_point(aes(x=5.1, y=1.9), shape=1, size=20, color="blue") +
  annotate("segment", x=x_coords[c(3, 4)], y=y_coords[c(3, 4)], 
           xend=x_coords[c(6, 1)], yend=y_coords[c(6, 1)],
           linetype="dashed") +
  ggtitle("Single-Linkage")
```

With **Average-Linkage**,  the distance between two clusters is determined by the average of all distances pointwise distances between clusters.  In the plot, this is estimated by "eyeballing" the center of the clusters.  In reality, the algorithm operates differently.  

```{r, q18, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-data1a"}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

x_coords <- pull(iris_newsub, Petal.Length)
y_coords <- pull(iris_newsub, Petal.Width)
iris_newsub %>%
  ggplot() +
  aes(x=Petal.Length, y=Petal.Width, color=Species) +
  geom_point() +
  geom_point(aes(x=1.33, y=0.21), shape=1, size=10, color="red") +
  geom_point(aes(x=4.4, y=1.5), shape=1, size=20, color="green") + 
  geom_point(aes(x=5.1, y=1.9), shape=1, size=20, color="blue") +
  annotate("segment", x=c(4.4, 4.4), y=c(1.5, 1.5),
           xend=c(1.35, 5.1), yend=c(0.2,1.9),
           linetype="dashed") +
  ggtitle("Average Linkage")
```

Generally, **average-linkage** performs well for most applications, but there are certainly situations where other methods should be evaluated.  

## Scaling Variables  

For the `iris` data set, the variables are similar.  Each is a measurement of distance in centimeters.  However, this need not be the case.  Other potential variables for flower organs are weight, thickness and pigment.  In this case, the units between the variables are more difficult to compare.  

Here is an exagerated situation.  Given a data set with `height` and `weight` measurements for three men, Chris, Eric, and Adam. Height is measured in feet and weight is measured in kilograms.

```{r q31, exercise=TRUE, exercise.startover=TRUE}
ex_data2
```

The weight of these men is very similar, but their heights are very different.  Eric is a giant at 8 feet tall!  

We can determine the Euclidean distances for the men.  

```{r q32, exercise=TRUE, exercise.startover=TRUE}
ex_data2_dist <- ex_data2 %>% 
  select(height:weight) %>%
  dist(method="euclidean") %>%
  as.matrix()
rownames(ex_data2_dist) <- colnames(ex_data2_dist) <- pull(ex_data2, name)
ex_data2_dist
```

As you can see, the **distance** between Adam and Chris is `4`, and the **distance** between Adam and Eric is `2.24`. Based on these distances, we can conclude that Adam is more similar to Eric than to Chris. However, in reality this is definitely not true!  

This problem is that height and weight have different units and scales.  A unit-wise shift in height is more significant than a unit-wise shift in weight.  

To account for this problem, we need to **scale** the variables. There are two common methods: min-max scaling and standardization. The latter is also called **z-score standardization**, and it is the more commonly used method.  

To standardize a numeric variable, we need to calculate the **mean** and **standard deviation** for each variable. Then, we subtract the **mean** from each observation and divide by the **standard deviation**.  

In **R** you can perform the standardization for each numeric variable in a data set with `scale` function. Importantly, `scale` assumes that your variables are in columns!  

The function `scale` requires three **arguments**,  

* `x` is the tibble, data frame or matrix with numeric variables.
* `center = TRUE` to subtract the **mean** for each variable.
* `scale = TRUE` to divide by the **standard deviation** for each variable.  

For example, the code below will scale the `height` and `weight` variables in `ex_data2` and calculate the new distance matrix.

```{r q33, exercise=TRUE, exercise.startover=TRUE}
ex_data2_dist <- ex_data2 %>% 
  select(height:weight) %>%
  scale() %>%
  dist(method="euclidean") %>%
  as.matrix()
rownames(ex_data2_dist) <- colnames(ex_data2_dist) <- pull(ex_data2, name)
ex_data2_dist
```

Now, the distance between Adam and Chris is 1.512 which is smaller than the distance between Adam and Eric, 1.773. Eric is now clearly the standout in the data set.  Problem fixed!

## Hierarchical Clustering  

Given the distances between observations, how do you join these into similar groups or clusters?  For a given data set, there are two extreme possibilities.  Each observation belongs to its own unique cluster, or all observations belong in a single cluster.  In reality, the best answer lies somewhere in between.  Hierarchical clustering is the general method that explores the possibilites between these two extremes.  

There are two types of hierarchical clustering algorithms: **Agglomerative Clustering** and **Divisive Clustering**.  With **agglomerative clustering**, initially each observation forms its own group.  The first group is formed by joining the two most similar observations into a cluster.  The selected **linkage method** is used to recalculate the distances.  Importantly, the distances for the observations that have been joined are replaced by the distances for their cluster.  Thus, as **agglomerative clustering** proceeds, the number of observations decreases as they are joined into clusters.  The process stops when all observations have been grouped into a single cluster.  

As the name suggests, **divisive clustering** works in the opposite direction.  All observations are initially grouped into a single cluster, and observations are "split" out of this cluster until each observation is the sole member of its own cluster.  

For most purposes, **agglomerative clustering** is suitable.  

The result of **hierarchical clustering** is often visualized as a dendrogram or tree.  Branches represent each observation and the distance (often labelled height) at which they are joined to other observations or cluster.  Critically, the arrangement of observations at the tips of the branches is not important.  The important information is the structure of the tree and the distances at which clusters are formed.  

Let's try a simple example with a subset of the `iris` data.  First, we will determine the Euclidean distances between the samples.  Scaling is probably not critical here, but we will do it anyway.  

Much of the code in the chunk below is necessary to create the highlighted table.  It is shown for the curious.  

```{r q34, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_dist <- iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  as.matrix() %>%
  signif(digits=4)

rownames(iris_dist) <- colnames(iris_dist) <- pull(iris_newsub, plant_id)
iris_dist %>%
  as_tibble() %>%
  mutate(id = pull(iris_newsub, plant_id), 
         plant_150 = cell_spec(plant_150, "html", 
                               color = ifelse(plant_150 < 0.8 & plant_150 != 0, "red", "black"))) %>%
  select(id, plant_1:plant_150) %>%
  kable(escape=FALSE) %>%
  kable_styling("striped")
```

The smallest distance which occurs between plants 62 and 150 has been highlighted red.  These two observation should form the first cluster in the tree.  

Let's do the clustering and indicate this distance with a dashed red line.  Note that we are using **complete-linkage**.  With this method, the maximal distance to a cluster is used when joining.  

```{r q35a, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  hclust(method="complete") %>%
  plot(labels=pull(iris_newsub, plant_id)) %>%
  abline(h=0.6246, lty=2, col="red")
```

Let's examine the distance matrix again to understand the next join that occurs between plants 55 and 148.  

```{r q34b, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_dist <- iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  as.matrix() %>%
  signif(digits=4)

rownames(iris_dist) <- colnames(iris_dist) <- pull(iris_newsub, plant_id)
iris_dist %>%
  as_tibble() %>%
  mutate(id = pull(iris_newsub, plant_id), 
         plant_150 = cell_spec(plant_150, "html", 
                               color = ifelse(plant_150 == 0.6246, "blue", "black")),
         plant_148 = cell_spec(plant_148, "html", color = ifelse(plant_148 > 0 & plant_148 <=1.17, "red", "black"))) %>%
  select(id, plant_1:plant_150) %>%
  kable(escape=FALSE) %>%
  kable_styling("striped")
```

This joining is a bit more difficult to understand.  The smallest distance for `plant_148` is with `plant_150`.  However, `plant_150` is in a cluster with `plant_62`.  To join this cluster, **complete-linkage** uses the maximum distance between observations.  This distance between `plant_62` and `plant_148` is `1.17` which is larger than the distance between `plant_55` and `plant_148`.  So, a cluster is formed between `plant_55` and `plant_148` and not between `plant_148` and the first cluster.  

```{r q35b, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  hclust(method="complete") %>%
  plot(labels=pull(iris_newsub, plant_id)) %>%
  abline(h=1.094, lty=2, col="red")
```

The next join is between the two existing clusters.  If you inspect all the distances between the four plants in the two clusters, they are all smaller than any distance with `plant_1` or `plant_3`.  So, the next join is formed at the maximum distance between the four plants in the two clusters.  

```{r q36, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  hclust(method="complete") %>%
  plot(labels=pull(iris_newsub, plant_id)) %>%
  abline(h=1.257, lty=2, col="red")
```

The next join occurs at the distance between `plant_1` and `plant_3`.  

```{r q37, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  hclust(method="complete") %>%
  plot(labels=pull(iris_newsub, plant_id)) %>%
  abline(h=1.364, lty=2, col="red")
```

Finally, the join to form a single cluster occurs at the maximum distance between the observations, the distance between `plant_1` and `plant_55`.  

```{r q38a, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  hclust(method="complete") %>%
  plot(labels=pull(iris_newsub, plant_id)) %>%
  abline(h=4.235, lty=2, col="red")
```

Remember, with cluster dendrograms, the information is in how the tree is formed.  All branch points can be rotated along the x-axis, and you have the same tree.  The distance between observations is determined vertically and not horizontally.  For example, `plant_148` and `plant_1` are plotted adjacently on the x-axis, but their actual distance is `4.183`!  

If you don't think that the **linkage** method is important, compare the tree formed with **single-linkage** below to the tree formed with **complete-linkage** above.  

```{r q38b, exercise=TRUE, exercise.startover=TRUE}
iris_newsub <- iris %>%
  slice(c(1, 3, 55, 62, 148, 150))

iris_newsub %>%
  select(Sepal.Length:Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  dist(method="euclidean") %>%
  hclust(method="single") %>%
  plot(labels=pull(iris_newsub, plant_id))
```

This tree is quite different because `plant_148` was allowed to join the first existing cluster.  Then, `plant_55` joined this cluster as well.  Here, the impact was minimal.  For larger, complex data sets, this can have a definite impact on the final clustering.  

## Clustering the Complete Iris Data

Above, we used a small subset of the `iris` data set.  Here, we will use hierarchical clustering to see if we can correctly cluster the plants by their species given the measurements of floral organs.  

```{r q51, exercise=TRUE, exercise.startover=TRUE}
iris
```

Before you leap into clustering, it is good practice to do some preliminary analysis of the data.  Data visualizations are very useful for this.  

There are four quantitative variables, so let's use scatterplots to see if there is any hint of correlation between variables or clustering of samples.  

The `pairs` function from the **graphics** package can plot all possible pairwise scatterplots from a given **object**.  

The numeric variables from `iris` can be selected and piped to `pairs`.  

```{r q52, exercise=TRUE, exercise.startover=TRUE}
iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  pairs()
```

The pairwise scatterplots show evidence of at least two clusters. However, since we know there are actually three clusters (3 species), we hope to resolve the groups better by using all quantitative variables with hierarchical clustering.

Before actually performing the hierarchical clustering, let's see whether the four quantitative variables have similiar scale or not by comparing their distributions with a boxplot.  

To do so, first we need to transform the dataset with the `pivot_longer` function, so that it has one column indicates the trait and one column stores the measurement of the corresponding trait.  

```{r q53a, exercise=TRUE, exercise.startover=TRUE}
iris %>% 
  pivot_longer(cols= Sepal.Length:Petal.Width, values_to="measurement", names_to="trait")
```

Then visualize as a boxplot.  

```{r q53b, exercise=TRUE, exercise.startover=TRUE}
iris %>% 
  pivot_longer(cols= Sepal.Length:Petal.Width, values_to="measurement", names_to="trait") %>%
  ggplot() +
  geom_boxplot(mapping=aes(x=trait, y=measurement))
```

Not surprisingly, the lengths tend to be greater than the widths.  The `traits` also seem to have different variability (indicated by the range of the data), so we should also account for this, e.g. a small shift in the width of a sepal is likely more meaningful than a small shift in the length.

We need to scale the four variables with `scale`. Remember that the `scale` function will return a numeric matrix instead of a **tibble**, so we need to transform it back to a **tibble** with `as_tibble`.


```{r q54, exercise=TRUE, exercise.startover=TRUE}
iris_zscores <- iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  scale() %>%
  as_tibble()
iris_zscores
```


```{r prepare-zscores, include=FALSE}
iris_zscores <- iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  scale() %>%
  as_tibble()
```

If we repeat the boxplot, we will see that the variables are on a similar scale now. 

```{r q55, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-zscores"}
iris_zscores %>% 
  pivot_longer(cols=everything(), values_to="measurement", names_to="treatment") %>%
  ggplot() +
  aes(x=treatment, y=measurement) +
  geom_boxplot()
```

After the steps above (ie, data visualizations and data preprocessing) we are ready to perform hierarchical clustering as we did with the smaller data set.  

This time, we will use the plant species in the label.  

```{r q56, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-zscores"}
iris_dist <- dist(iris_zscores, method="euclidean")
iris_hclust <- hclust(iris_dist, method="average")
iris_hclust$labels <- str_c(pull(iris, Species), 1:nrow(iris), sep="_")
plot(iris_hclust, cex=0.50, main="Clustering of Four Iris Variables")
```

It is nearly impossible to assess the clustering because the labels overlap. We could make the plot larger, but it is more helpful to label each species with a specific color. The package **RColorBrewer** has multiple color palettes that have been optimized for specific purposes.  

```{r prepare-cluster_iris, include=FALSE}
iris_zscores <- iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  scale() %>%
  as_tibble()
iris_dist <- dist(iris_zscores, method="euclidean")
iris_hclust <- hclust(iris_dist, method="average")
iris_hclust$labels <- str_c(pull(iris, Species), 1:nrow(iris), sep="_")
```

We will improve this visualization by adding color to the plant labels.  Each plant will have a color that corresponds to its species.  Given the available plotting methods for dendrograms, this is not a trivial task.  

First, we will create a pallete of suitable colors using the package **RColorBrewer**.  The function `brewer.pal` will return a specified number of colors from a selection of available palettes.  Here we will get three colors from the `Dark2` palette intended for qualitative data.  

We then need to create a vector of specific colors from this palette that correspond to the plant species.  We can use a "trick" with **factors** to do this.  **Factors** are actually stored as integers with an associated label.  Generally, you see the label and not the underlying integer.  Regardless, we can use this fact to rapidly create our color vector with this expression:  

`sam_cols <- iris_cols[as.factor(iris$Species)]`  

To add the colors to the plot, we will suppress the default labels.  Instead, we will add marginal text with `mtext` and color this text with the species colors.  Note that this text must be added in the order returned by `hclust`.  

Finally, we will create a color legend with `legend`.  

This may seem unnecessarily complicated, but there is still no good way to plot a dendrogram with ggplot.  

```{r q58, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-cluster_iris"}
iris_cols <- brewer.pal(n=3, name="Dark2")
sam_cols <- iris_cols[as.factor(iris$Species)]

plot(as.dendrogram(iris_hclust), type="rectangle", leaflab="none", cex.lab=2, 
     main="Clustering of Four Iris Variables")

mtext(iris$Species[iris_hclust$order], col=sam_cols[iris_hclust$order], 
      side=1, at=1:nrow(iris), line=0.5, cex=0.5, las=2, family="mono")

legend("topright", legend=c("setosa", "versicolor", "viginica"), fill=iris_cols)
```

The dendrogram shows that there is good clustering of setosa (no overlaps with other species), but versicolor and virginica are not resolved. Let's go back and look at our scatterplot again, but with the points color-coded by specifying the `col` argument.

```{r q59, exercise=TRUE, exercise.startover=TRUE}
iris_cols <- brewer.pal(n=3, name="Dark2")
sam_cols <- iris_cols[as.factor(iris$Species)]
iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  pairs(col=sam_cols, main="Pairs Plot of Iris Variables")
```

The variables `Sepal.Length` and `Sepal.Width` do not resolve versicolor and virginica, since there are overlaps betwee the two (For example, see the plot on the first row and the second column).  

In contrast, if you look at the plot on the third row and the last column, `Petal.Length` and `Petal.Width` show resolution of all three species. This suggests us to try clustering on only those two variables.

```{r prepare-510}
iris_cols <- brewer.pal(n=3, name="Dark2")
sam_cols <- iris_cols[as.factor(iris$Species)]
iris_zscores <- iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  scale() %>%
  as_tibble()
```


```{r q510, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-510"}
iris_dist2 <- iris_zscores %>%
              select(Petal.Length, Petal.Width) %>%
              dist(method="euclidean")
iris_hclust2 <- hclust(iris_dist2, method="average")
iris_hclust2$labels <- str_c(pull(iris, Species), 1:nrow(iris), sep="_")

plot(as.dendrogram(iris_hclust2), type="rectangle", leaflab="none", cex.lab=2, main="Clustering of Two Iris Variables")

mtext(iris$Species[iris_hclust2$order], col=sam_cols[iris_hclust2$order], side=1, at=1:nrow(iris), 
      line=0.5, cex=0.5, las=2, family="mono")
      
legend("topright", legend=c("setosa", "versicolor", "viginica"), fill=iris_cols)
```

Even though it is still not perfect, but now only 3 obseravtions are incorrectly clustered along species lines.  This demonstrates that more data is not necessarily always better!  

## Extracting Groups from Hierarchical Clustering  

We have performed the hierarchical clustering and visualized the groups, but how can we define actual clusters?  

The first step is to visually assess your tree.  For the `iris` data, we have three groups formed at a height of about 0.8.  

This can be shown on the plot.  

```{r q511a, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-510"}
iris_dist2 <- iris_zscores %>%
              select(Petal.Length, Petal.Width) %>%
              dist(method="euclidean")
iris_hclust2 <- hclust(iris_dist2, method="average")
iris_hclust2$labels <- str_c(pull(iris, Species), 1:nrow(iris), sep="_")

plot(as.dendrogram(iris_hclust2), type="rectangle", leaflab="none", cex.lab=2, main="Clustering of Two Iris Variables")
abline(h=0.8, lty=2, col="red")

mtext(iris$Species[iris_hclust2$order], col=sam_cols[iris_hclust2$order], side=1, at=1:nrow(iris), 
      line=0.5, cex=0.5, las=2, family="mono")
      
legend("topright", legend=c("setosa", "versicolor", "viginica"), fill=iris_cols)
```

Imagine if we "cut" the branches of the tree at this height, three branches will fall and define three clusters with each leaf an individual observation.  

The function `cutree` will actually perform this operation on an **hclust** object and return the results.  To use `cutree` you must either specify the number of groups that you want, `k`.  Alternatively, you can specify the height, `h`, at which to cut the tree.  

The result is a vector of group numbers for the input data.  Importantly, the group number is returned in the same order as the original input data, NOT in the order shown in the tree!  

```{r prepare-511}
iris_zscores <- iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  scale() %>%
  as_tibble()
iris_dist2 <- iris_zscores %>%
              select(Petal.Length, Petal.Width) %>%
              dist(method="euclidean")
iris_hclust2 <- hclust(iris_dist2, method="average")
iris_hclust2$labels <- paste(iris$Species, rownames(iris))
```


```{r q511b, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-511"}
iris_cuts2 <- cutree(iris_hclust2, k=3)
head(iris_cuts2)
```

We can add this group number to the original tibble.  

```{r q512, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-511"}
iris_cuts2 <- cutree(iris_hclust2, k=3)
iris <- iris %>%
  mutate(group=cutree(iris_hclust2, k=3))

iris %>%
  ggplot() +
  aes(x=group, fill=Species) +
  geom_bar(position="stack") +
  ggtitle("Clustering Results for Iris")
```

The clustering of the iris samples appears to be quite strong, with only three samples incorrectly clustered based on species.  

## Heatmaps

A common way to visualize the results of a clustering is with a heatmap.  A heatmap is a picture of your data where the color or intensity of color, indicates some quantitative value.  Typically, these values are binned as in a histogram because the human eye can only distinguish so many colors. There are multiple heatmap functions and packages for **R**.  One of the best is the **pheatmap** package.

Refer to the help page for `pheatmap`. This function has many arguments that affect the appearance of the plot, including hierarchical clustering of the data by row and/or by column.  Let's try `pheatmap` with the default values. Since we already decided to use `Petal.Length` and `Petal.Width` for the clustering algorithm, we will do the same thing here for the heatmap.

```{r q71, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-zscores"}
iris_zscores2 <- iris_zscores %>%
                  select(Petal.Length, Petal.Width)

pheatmap(mat = iris_zscores2)
```

This heatmap has two columns, `Petal.Length` and `Petal.Width`.  It also has the same number of rows as the data set. Each row of the heatmap represents an observation from the data set, but the order has changed.  

By default, `pheatmap` reorders both the columns and rows by hierarchical clustering.  Dendrograms are actually shown in both margins to indicate the clustering.  

We can change the default values for `pheatmap` to manipulate this clustering.  First, there is no need to reorder the columns, and the rownames are too small to be legible.  

```{r prepare-7}
iris_zscores <- iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  scale() %>%
  as_tibble()

iris_zscores2 <- iris_zscores %>%
                  select(Petal.Length, Petal.Width)
```


```{r q72, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-7"}
pheatmap(mat = iris_zscores2, 
         cluster_cols=FALSE, 
         show_rownames=FALSE)
```

If you examine the default values for `pheatmap`, the clustering distance is assigned `"euclidean"` and the linkage method is `"complete"`, and we would like to change the linkage method to `average`.


```{r q73, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-7"}
pheatmap(iris_zscores2, 
         cluster_cols=FALSE, 
         show_rownames=FALSE, 
         clustering_method="average") 
```


If you compare this heatmap to the previous one, there are definite differences. You can use the argument `cutree_rows` to specify a number of clusters, and the resulting heatmap will have small gaps between the clusters.  


Since we know there are three clusters, we should set `cutree_rows` to be 3. However, if you have no idea about how many clusters there are, you can change the `cutree_row` until you get a reasonable number of clusters judging by eye, such that for each column, the observations in the same cluster will have similar color.

```{r q74, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-7"}
pheatmap(iris_zscores2, 
         cluster_cols=FALSE, 
         show_rownames=FALSE, 
         clustering_method="average", 
         cutree_rows=3) 
```

The `pheatmap` function will store the results of the hierarchical clustering so that you can extract the data. The results for the hierarchical clustering are stored in the `tree_row` attribute of `my_map`.  

You can set `silent=TRUE` to not plot the heatmap, if you don't want to plot the heatmap.

```{r q75, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-7"}
my_map <- pheatmap(iris_zscores2, 
                   cluster_cols=FALSE, 
                   show_rownames=FALSE, 
                   clustering_method="average", 
                   cutree_rows=3,
                   silent=TRUE) 
                   
                   
my_map$tree_row
```

```{r prepare-76}
iris_zscores <- iris %>% 
  select(Sepal.Length:Petal.Width) %>%
  scale() %>%
  as_tibble()

iris_zscores2 <- iris_zscores %>%
                  select(Petal.Length, Petal.Width)

my_map <-pheatmap(iris_zscores2, 
                   cluster_cols=FALSE, 
                   show_rownames=FALSE, 
                   clustering_method="average", 
                   cutree_rows=3,
                   silent=TRUE)
```

You can cut this tree with `cutree()` using the same value that you used for `cutree_row()` in `pheatmap()`. And get the same contingency table from the previous section.

```{r q76, exercise=TRUE, exercise.startover=TRUE, exercise.setup="prepare-76"}
my_clusters <- cutree(my_map$tree_row, k=3)

table(iris$Species, my_clusters)
```

In fact, `pheatmap` uses `hclust` to do the clustering.  So, whether you use `hclust` directly or via `pheatmap`, you should get the same results.  

## Adding Annotation Data to a Heat Map  

You may have additional data about your observations that you want to add to your heatmap.  For example, we know the `Species` for the iris plants.  

The argument `annotation_row` will accept a **data frame** with annotation data for your observations.  Importanly, this must be a **data frame** with row names that match row names in your input data.  

We need to make two adjustments to our code.  First, we need to create an actual data matrix with row names.  Then, we need to make a **data frame** with the row annotation data.  

The result will be a colored bar between the dendrogram and the heat map that indicates the species of the observations.  

```{r q77, exercise=TRUE, exercise.startover=TRUE}
iris_mat <- iris %>%
  select(Petal.Length, Petal.Width) %>%
  scale(center=TRUE, scale=TRUE) %>%
  as.matrix()
rownames(iris_mat) <- pull(iris, plant_id)

row_annot <- data.frame(Species = pull(iris, Species))
rownames(row_annot) <- pull(iris, plant_id)

pheatmap(iris_mat, 
         cluster_cols=FALSE, 
         show_rownames=FALSE, 
         clustering_method="average", 
         cutree_rows=3, 
         annotation_row=row_annot)
```

There is much more to explore with hierarchical clustering.  It is a very useful technique with many applications and methods.  

## Session Information  

```{r}
sessionInfo()
```





